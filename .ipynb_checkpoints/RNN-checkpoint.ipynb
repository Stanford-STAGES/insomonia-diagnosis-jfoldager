{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Jonathan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model,Sequential,model_from_json\n",
    "from keras.layers import Input,BatchNormalization,Lambda,Layer,Dense,Dropout, Activation, Flatten,Embedding, LSTM\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model,np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "#from xgboost import XGBClassifier\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "pth = 'Data/_features/SHHS/'\n",
    "file = '205802.h5'\n",
    "ins = np.array([])\n",
    "armodelMaxShape = [0,0]\n",
    "spectralMaxShape = [0,0]\n",
    "temporalMaxShape = [0,0]\n",
    "if os.path.isdir(pth):\n",
    "    files = [f.name for f in os.scandir(pth)]   \n",
    "    for file in files:\n",
    "        filename = pth + file\n",
    "        h5file = h5py.File(filename, 'r')\n",
    "        #Get the HDF5 group\n",
    "        #demographics = h5file['demographics'].value.transpose()\n",
    "        armodel = h5file['armodel'].shape[::-1]\n",
    "        spectral = h5file['spectral'].shape[::-1]\n",
    "        temporal = h5file['temporal'].shape[::-1]\n",
    "        if armodel[0] > armodelMaxShape[0]:\n",
    "            armodelMaxShape = armodel\n",
    "        if spectral[0] > spectralMaxShape[0]:\n",
    "            spectralMaxShape = spectral\n",
    "        if temporal[0] > temporalMaxShape[0]:\n",
    "            temporalMaxShape = temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6414, 5)\n",
      "(6414, 8)\n",
      "(6414, 13)\n"
     ]
    }
   ],
   "source": [
    "print(armodelMaxShape)\n",
    "print(spectralMaxShape)\n",
    "print(temporalMaxShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 28\n",
      "Trainable params: 28\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, input_shape=(armodelMaxShape)))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n",
      "(16, 6414, 5)\n",
      "[[[-1.75214  1.68093 -1.22144  0.55207 -0.2254 ]\n",
      "  [-1.60133  1.20827 -0.69916  0.16871 -0.04224]\n",
      "  [-2.15018  2.59032 -2.23295  1.22943 -0.4079 ]\n",
      "  ..., \n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]]\n",
      "\n",
      " [[-2.48036  3.34749 -3.05106  1.78569 -0.5615 ]\n",
      "  [-2.58718  3.50417 -3.20397  1.87044 -0.56981]\n",
      "  [-2.17514  2.61348 -2.19942  1.14485 -0.33878]\n",
      "  ..., \n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]]\n",
      "\n",
      " [[-1.65168  1.22885 -0.50092  0.02432  0.02658]\n",
      "  [-1.67402  1.20928 -0.46595 -0.05868  0.08384]\n",
      "  [-2.25397  3.06991 -2.72415  1.496   -0.4944 ]\n",
      "  ..., \n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]]\n",
      "\n",
      " ..., \n",
      " [[-2.27517  3.11719 -2.93744  1.79816 -0.68662]\n",
      "  [-2.26462  2.65204 -2.07258  0.9418  -0.23785]\n",
      "  [-1.79304  1.56711 -0.90995  0.19395 -0.02802]\n",
      "  ..., \n",
      "  [-2.21692  2.93049 -2.62284  1.74637 -0.55799]\n",
      "  [-1.99884  2.43764 -1.91779  1.19366 -0.30098]\n",
      "  [-2.26176  2.79897 -2.37579  1.54894 -0.55783]]\n",
      "\n",
      " [[-2.46111  2.99359 -2.5489   1.37189 -0.34731]\n",
      "  [-2.63433  3.47897 -3.10995  1.75776 -0.4857 ]\n",
      "  [-2.31952  2.85219 -2.47001  1.35102 -0.38265]\n",
      "  ..., \n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]]\n",
      "\n",
      " [[-2.29178  3.02778 -2.82602  1.65518 -0.54754]\n",
      "  [-2.02732  2.43204 -2.1411   1.20753 -0.45125]\n",
      "  [-1.76036  1.73822 -1.30317  0.48277 -0.13438]\n",
      "  ..., \n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.     ]]]\n"
     ]
    }
   ],
   "source": [
    "batchSize = 16\n",
    "\n",
    "X = np.zeros((batchSize,armodelMaxShape[0],armodelMaxShape[1]))\n",
    "Y = np.empty((batchSize,1))\n",
    "\n",
    "files = [f.name for f in os.scandir(pth)]   \n",
    "for i, file in enumerate(files):\n",
    "    filename = pth + file\n",
    "    h5file = h5py.File(filename, 'r')\n",
    "\n",
    "    y = h5file['demographics'].value[-1]\n",
    "    x = h5file['armodel'].value.transpose()\n",
    "    \n",
    "    Y[i,:] = y\n",
    "    X[i,:x.shape[0],:] = x \n",
    "    \n",
    "    if i == batchSize - 1:\n",
    "        break\n",
    "        \n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "model.fit(X, Y, epochs=32, batch_size=8, verbose=2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6414, 5) (6,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main():\n",
    "    ## Logistic regression (sckitlearn) first\n",
    "    #logisticregression()\n",
    "    ## Random forest (sckitlearn) second (and feature selection)\n",
    "    #randomforest()\n",
    "    #lightgbm()\n",
    "    ## Deep learning\n",
    "    #feedforwardnetwork()\n",
    "    fivesecfeedforwardnetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def logisticregression():\n",
    "    clear = lambda: os.system('cls')\n",
    "    clear() # remove warnings\n",
    "    ################################ Load data ############################\n",
    "    X = np.loadtxt(\"../../../MATLAB/XSHHS.csv\", delimiter=\",\")\n",
    "    Y = np.loadtxt(\"../../../MATLAB/YSHHS.csv\", delimiter=\",\")\n",
    "    predicted = cross_validation.cross_val_predict(LogisticRegression(), X, Y, cv=10)\n",
    "    print(metrics.accuracy_score(Y, predicted))\n",
    "    print(metrics.classification_report(Y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest():\n",
    "    clear = lambda: os.system('cls')\n",
    "    clear() # remove warnings\n",
    "    ################################ Load data ############################\n",
    "    X = np.loadtxt(\"../../../MATLAB/XSHHS.csv\", delimiter=\",\")\n",
    "    Y = np.loadtxt(\"../../../MATLAB/YSHHS.csv\", delimiter=\",\")\n",
    "    text_file = open(\"../../../MATLAB/featurenames.csv\", \"r\")\n",
    "    featurenames = text_file.read().split(',')\n",
    "\n",
    "    #accs = []\n",
    "    #samples = []\n",
    "    #sizes = np.arange(0.8,0.95,0.05)\n",
    "    #for s in sizes:\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, train_size=0.9)\n",
    "    acc = []\n",
    "    for idx_split, (idx_train, idx_test) in enumerate(sss.split(X, Y)):\n",
    "        rfs = RandomForestClassifier(n_estimators=10)\n",
    "        Xtrain = X[idx_train] \n",
    "        Ytrain = Y[idx_train]\n",
    "        Xtest = X[idx_test] \n",
    "        Ytest = Y[idx_test]\n",
    "        siz = len(idx_train)\n",
    "        rfs.fit(Xtrain, Ytrain)\n",
    "        y_pred = rfs.predict(Xtest)\n",
    "        acc.append(metrics.accuracy_score(Ytest, y_pred))\n",
    "        print(metrics.accuracy_score(Ytest, y_pred))\n",
    "    #accs.append(np.median(acc))\n",
    "    #samples.append(siz)\n",
    "    #print(samples,accs)\n",
    "\n",
    "    importances = rfs.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rfs.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = []\n",
    "    for idx in indices:\n",
    "        names.append(featurenames[idx])\n",
    "    print(\"Feature ranking:\")\n",
    "    \n",
    "    with open('Ranking.csv', 'w') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|',\n",
    "                        quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "        for f in range(X.shape[1]):\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, names[f], importances[indices[f]]))\n",
    "            a = names[f]\n",
    "            b = str(importances[indices[f]])\n",
    "            spamwriter.writerow([a, b])\n",
    "\n",
    "\n",
    "    #i_tree = 0\n",
    "    #for tree_in_forest in rfs.estimators_:\n",
    "    #    with open('tree_' + str(i_tree) + '.dot', 'w') as my_file:\n",
    "    #        my_file = tree.export_graphviz(tree_in_forest, out_file = my_file)\n",
    "\n",
    "    ## With XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm():\n",
    "    PARAMS_LGB = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"num_leaves\": 30,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"bagging_fraction\": 0.7,\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_seed\": 2018,\n",
    "    \"verbosity\": -1}\n",
    "\n",
    "    X = np.loadtxt(\"../../../MATLAB/XSHHS.csv\", delimiter=\",\")\n",
    "    Y = np.loadtxt(\"../../../MATLAB/YSHHS.csv\", delimiter=\",\")\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2)\n",
    "    train_data = lgb.Dataset(Xtrain, label=Ytrain)\n",
    "    test_data = lgb.Dataset(Xtest, label=Ytest)\n",
    "    num_round = 10\n",
    "    bst = lgb.train(PARAMS_LGB, train_data, num_round, valid_sets=[test_data])\n",
    "    train_data = lgb.Dataset(X, label=Y)\n",
    "    bst = lgb.cv(param, train_data, num_round, nfold = 10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def feedforwardnetwork():\n",
    "    clear = lambda: os.system('cls')\n",
    "    clear() # remove warnings\n",
    "    ################################ Load data ############################\n",
    "    X = np.loadtxt(\"../../../MATLAB/XSHHS.csv\", delimiter=\",\")\n",
    "    Y = np.loadtxt(\"../../../MATLAB/YSHHS.csv\", delimiter=\",\")\n",
    "    ############################ Create model ##########################\n",
    "    # baseline\n",
    "    #neurons = [\n",
    "    #    1024,\n",
    "    #    256,\n",
    "    #    256,\n",
    "    #    256,\n",
    "    #    256,\n",
    "    #    512,\n",
    "    #    256,\n",
    "    #    512\n",
    "    #           ]\n",
    "\n",
    "    neurons = [\n",
    "        2048,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "               ]\n",
    "\n",
    "    #neurons = [1024]\n",
    "\n",
    "    actFuncs = [\n",
    "        'relu',\n",
    "        'relu',\n",
    "        'relu',\n",
    "        'relu',\n",
    "                ]\n",
    "\n",
    "    if len(actFuncs) != len(neurons):\n",
    "        raise ValueError('No. hidden layers not consistent!')\n",
    "\n",
    "    nHidLays = len(neurons)\n",
    "    model = Sequential()\n",
    "    ################################# Input layer\n",
    "    ################################# #################################\n",
    "    model.add(Dense(X.shape[1],input_dim = X.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    reg = 0.01\n",
    "\n",
    "    ################################# Hidden layer(s)\n",
    "    ################################# #################################\n",
    "    for l in range(nHidLays):\n",
    "        model.add(Dense(neurons[l], activation=actFuncs[l], kernel_regularizer=l2(reg)))\n",
    "\n",
    "    ################################# Output layer\n",
    "    ################################# #################################\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "\n",
    "    \n",
    "    mdlName = str.replace('./model_{}'.format(datetime.now().isoformat()[11:19]),':','') \n",
    "    ## Callbacks\n",
    "    tensorboard = TensorBoard(log_dir=\"./Graph/{}\".format(datetime.now().isoformat()[:4]), \n",
    "                              write_images=True)\n",
    "    checkpoint = ModelCheckpoint(mdlName + '.h5',\n",
    "                                monitor='val_acc',\n",
    "                                verbose=0,\n",
    "                                mode='max',\n",
    "                                save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=20,\n",
    "                              verbose=0, \n",
    "                              mode='auto')\n",
    "    # Optimizers\n",
    "    #opt = Adam(lr=0.01, decay=0.0)\n",
    "    opt = SGD(lr=0.001, momentum=0.9, decay=0.0)\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    ## Save model to json\n",
    "    #model_json = model.to_json()\n",
    "    #with open(mdlName + \".json\", \"w\") as json_file:\n",
    "    #    json_file.write(model_json)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.4)\n",
    "\n",
    "    # Split validation into validation and test\n",
    "    Xval, Xtest, Yval, Ytest = train_test_split(Xtest, Ytest, test_size=0.5)\n",
    "\n",
    "    ## Train\n",
    "    model.fit(Xtrain, Ytrain, \n",
    "              validation_data=(Xval,Yval), \n",
    "              epochs=1000, \n",
    "              batch_size=16, \n",
    "              callbacks=[tensorboard,checkpoint,earlystop],\n",
    "              verbose=2)\n",
    "    \n",
    "    # Save test,validation and predictions\n",
    "    #testData = np.append(Xtest,Ytest,axis=1)\n",
    "    #np.savetxt(mdlName + '_test.csv', testData)\n",
    "\n",
    "    #valData = np.append(Xval,Yval,axis=1)\n",
    "    #np.savetxt(mdlName + '_val.csv', valData)\n",
    "\n",
    "    #DisplayBestModel(mdlName)\n",
    "\n",
    "def DisplayBestModel(file):\n",
    "    # load json and create model\n",
    "    json_file = open(file + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(file + '.h5')\n",
    "    opt = Adam(lr=0.001, decay=0.0)\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    data = np.loadtxt(file + '_val.csv')\n",
    "    X = data[:,:-2]\n",
    "    Y = data[:,-2:]\n",
    "    score = loaded_model.evaluate(X, Y)\n",
    "    print(\"%s: %.2f%%\" % ('Validation accuracy', score[1] * 100))\n",
    "    data = np.loadtxt(file + '_test.csv')\n",
    "    X = data[:,:-2]\n",
    "    Y = data[:,-2:]\n",
    "    score = loaded_model.evaluate(X, Y)\n",
    "    print(\"%s: %.2f%%\" % ('Test accuracy', score[1] * 100))\n",
    "\n",
    "def Metrics():\n",
    "    #file = '../../4.6/model_115405_75'\n",
    "    file = 'model_104420'\n",
    "    # load json and create model\n",
    "    json_file = open(file + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(file + '.h5')\n",
    "    opt = Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    data = np.loadtxt(file + '_test.csv')\n",
    "    Xval = data[:,:-2]\n",
    "    Yval = data[:,-2:]\n",
    "    valAcc = model.evaluate(Xval, Yval)\n",
    "    pC1 = model.predict(Xval)[:,0]\n",
    "    true1 = Yval[:,0]\n",
    "    true0 = Yval[:,1]\n",
    "    t = 0\n",
    "    i = 0\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "\n",
    "    FPR, TPR, T = roc_curve(true1,pC1)\n",
    "    plt.figure()\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.show()\n",
    "\n",
    "    F1 = []\n",
    "    for t in T:\n",
    "        c1 = (pC1 >= t) * 1\n",
    "        F1.append(f1_score(true1,c1))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(F1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BestXY():\n",
    "    X = np.loadtxt(\"../../../MATLAB/X.csv\", delimiter=\",\")\n",
    "    Y = np.loadtxt(\"../../../MATLAB/Y.csv\", delimiter=\",\")\n",
    "    Xz = X #stats.zscore(X)\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    # convert integers to dummy variables (i.e.  one hot encoded)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    ############################ Create model ##########################\n",
    "    # Fit the model\n",
    "    # baseline\n",
    "    reg = 0.01\n",
    "    neurons = [1024,\n",
    "               256,\n",
    "               256,\n",
    "               256,\n",
    "               256,\n",
    "               512,\n",
    "               256,\n",
    "               512]\n",
    "\n",
    "    actFuncs = ['relu',\n",
    "                'relu',\n",
    "                'relu',\n",
    "                'relu',\n",
    "                'relu',\n",
    "                'relu',\n",
    "                'relu',\n",
    "                'relu']\n",
    "\n",
    "    if len(actFuncs) != len(neurons):\n",
    "        raise ValueError('No. hidden layers not consistent!')\n",
    "\n",
    "    nHidLays = len(neurons)\n",
    "    model = Sequential()\n",
    "    ################################# Input layer\n",
    "    ################################# #################################\n",
    "    model.add(Dense(X.shape[1],input_dim= X.shape[1], activation='linear'))\n",
    "    #model.add(Dropout(0.5))\n",
    "\n",
    "    ################################# Hidden layer(s)\n",
    "    ################################# #################################\n",
    "    for l in range(nHidLays):\n",
    "        #model.add(BatchNormalization(axis=-1))\n",
    "        model.add(Dense(neurons[l], activation=actFuncs[l], kernel_regularizer=l2(reg)))\n",
    "\n",
    "    ################################# Output layer\n",
    "    ################################# #################################\n",
    "    model.add(Dense(dummy_y.shape[1], activation='softmax'))\n",
    "    print(model.summary())\n",
    "\n",
    "    #opt = Adam(lr=0.01, decay=0.1)\n",
    "    opt = SGD(lr=0.01)\n",
    "    \n",
    "    mdlName = str.replace('./model_{}'.format(datetime.now().isoformat()[11:19]),':','') \n",
    "    ## Callbacks\n",
    "    tensorboard = TensorBoard(log_dir=\"./Graph/{}\".format(datetime.now().isoformat()[:4]), \n",
    "                              write_images=True)\n",
    "    checkpoint = ModelCheckpoint(mdlName + '.h5',\n",
    "                                monitor='val_acc',\n",
    "                                verbose=0,\n",
    "                                mode='max',\n",
    "                                save_best_only=True)\n",
    "    earlystop = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=20,\n",
    "                              verbose=0, \n",
    "                              mode='auto')\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    # Save model to json\n",
    "    model_json = model.to_json()\n",
    "    with open(mdlName + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Split data into training and validation\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xz, dummy_y, test_size=0.4)\n",
    "\n",
    "    # Split validation into validation and test\n",
    "    Xval, Xtest, Yval, Ytest = train_test_split(Xtest, Ytest, test_size=0.5)\n",
    "\n",
    "    ## Train\n",
    "    model.fit(Xtrain, Ytrain, validation_data=(Xval,Yval), epochs=1000, batch_size=64, callbacks=[tensorboard,checkpoint])\n",
    "    \n",
    "    # Save test,validation and predictions\n",
    "    testData = np.append(Xtest,Ytest,axis=1)\n",
    "    np.savetxt(mdlName + '_test.csv', testData)\n",
    "\n",
    "    valData = np.append(Xval,Yval,axis=1)\n",
    "    np.savetxt(mdlName + '_val.csv', valData)\n",
    "\n",
    "    DisplayBestModel(mdlName)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
